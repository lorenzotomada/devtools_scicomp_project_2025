{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation \n",
    "**Disclaimer**: *this document is not meant to be neither a formal nor an exhaustive description of the iterative method to solve the eigenvalue problem. The aim of this text is only to provide the necessary and minimal information needed for the reader to understand the code implementation. We will use these pages to document, explain and justify the choice made from a numerical and scientific computing standpoint*.\n",
    "\n",
    "## Problem statement\n",
    "Given a matrix $\\boldsymbol{A} \\in \\mathbb{C}^{n, n}$, with $n \\in \\mathbb{N}$ the matrix dimension, the eigenvalue problem can be formulated as finding the eigenpair $\\{(\\lambda_i, \\boldsymbol{v}_i)\\}_{i=1} ^ n$, with $\\lambda_i \\in \\mathbb{C}$, $\\boldsymbol{v}_i \\in \\mathbb{C}^{n ,1}$ and $\\boldsymbol{v}_i \\ne \\boldsymbol{0}$ such that \n",
    "$$\\boldsymbol{A} \\boldsymbol{v}_i = \\lambda_i \\boldsymbol{v}_i, \\quad i=1, 2, ..., n $$\n",
    "\n",
    "$\\lambda_i$ are called eigenvalue, while $\\boldsymbol{v}_i$ are the corresponding eigenvectors. Theoretically, finding the matrix's eigenvalue is possible by imposing the following condition:\n",
    "$$ \\det(A-\\lambda I)=0 $$\n",
    "which lead to the well known characteristic polynomial of degree $n$ [[1](https://sissa-my.sharepoint.com/:b:/g/personal/glicausi_sissa_it/EeImU3eEs5hNoEF7hEnScxABpd0oMn5zOS8y-A3UJLoAGw?e=OO7IoE)]. The eigenvalue are the roots of the characteristic polynomial. Although correct, this apporach is not viable, due to the difficulties in both expressing the characteristic polynomial and find its roots, when the Matrix gets very large.\n",
    "\n",
    "Numerical methods takle the eigenproblem using a different strategy, most of the time being a iterative methods. Rather than aspire to get the exact solution, they seek after an approximation of the solution, which hopefully, under a set of conditions, converges to the exact solution.\n",
    "Among all the method devolped to solve the eigenproblem, the power method, along with its variants suct that the inverse power method and power method with shift, and the QR are the widest spread and the most used [[1](https://sissa-my.sharepoint.com/:b:/g/personal/glicausi_sissa_it/EeImU3eEs5hNoEF7hEnScxABpd0oMn5zOS8y-A3UJLoAGw?e=OO7IoE)].\n",
    "\n",
    "## Power method\n",
    "Let $\\boldsymbol{A}$ being a matrix and $\\lambda _i$ the eigenvalue already sorted accordingly to their module. If\n",
    "$$ |\\lambda_1| > |\\lambda _i | \\quad i=2, 3, ..., n$$\n",
    "then the power method allow to recover the eigenpair $(\\lambda_1, \\boldsymbol{v}_1)$ by applying iteratively, the following steps\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Power Method Algorithm} \\\\\n",
    "\\textbf{Input:} A \\in \\mathbb{C}^{n \\times n}, \\text{initial vector } x_0, \\text{ tolerance } \\text{tol}, \\text{ max iterations } \\text{max\\_iter} \\\\\n",
    "\\textbf{Output:} \\lambda \\text{ (dominant eigenvalue approximation), } x \\text{ (eigenvector approximation)} \\\\\n",
    "\n",
    "1.\\ \\text{Initialize } x = x_0 \\text{ (random or chosen guess)} \\\\\n",
    "2.\\ \\text{Normalize } x \\text{: } y^{(1)} = x/ \\| x\\| \\\\\n",
    "3.\\ \\lambda_{\\text{old}} = 0 \\\\\n",
    "\n",
    "4.\\ \\textbf{for } k = 1 \\textbf{ to } \\text{max\\_iter} \\textbf{ do} \\\\\n",
    "\\quad 5.\\ x^{(k+1)} = A \\cdot y^{(k)} \\\\\n",
    "\\quad 6.\\  y^{(k+1)} = x^{(k+1)}/ \\| x^{(k+1)}\\|  \\\\\n",
    "\\quad 7.\\  \\lambda^{(k+1)} =  (y^{(k+1)})^H \\boldsymbol{A} y^{(k+1)} \\\\\n",
    "\\quad 8.\\ \\textbf{if } |\\lambda^{(k+1)} - \\lambda^{(k)}| < \\text{tol} \\textbf{ then} \\\\\n",
    "\\quad \\quad \\text{Break (convergence reached)} \\\\\n",
    "\n",
    "9.\\ \\textbf{Return } \\lambda, x\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "By exploiting Matrix properties, it is also possible to find the eigenvalue with the smallest norm, and its associated eigenvector, or either the the eigenvalue closest to a given value.\n",
    "\n",
    "## QR algorithm\n",
    "Despite its simplicity, the power method has its limitations. First of all, it only allows one to get an eigenpair at a time. Secondly it does not exploit efficiently all the computation performed. At step $k$, the power method exploits only the information $ \\beta \\boldsymbol{A}^m \\boldsymbol{x^0}  $, where $\\beta$ is a given scaling factor. On the other hand, a quicker convergence and a better approximation of the eigenpair could be achieved by simply storing and making use of all the directions determined by the vectors   $\\boldsymbol{A}^i \\boldsymbol{x^0} $, with $ i $ up to the step $k$. Indeed, it is possible to collect them all in a matrix, named Krylov matrix $K^m(x^0)$:\n",
    "$$K^m(x^0)=[\\boldsymbol{x^0}, \\boldsymbol{A} \\boldsymbol{x^0}, ..., \\boldsymbol{A}^{m-1} \\boldsymbol{x^0}]$$  \n",
    "The space spanned by the columns of the Krylov matrix is called Krylov subspace $\\mathcal{K}^m$ [[2](https://sissa-my.sharepoint.com/my?id=%2Fpersonal%2Fglicausi%5Fsissa%5Fit%2FDocuments%2FPhD%2FDevelopment%20Tools%20for%20Scientific%20Computing%20%2D%20Project%20material%20and%20references%2F12%2Dkrylov%2Dsubspaces%2D1998%2Epdf&parent=%2Fpersonal%2Fglicausi%5Fsissa%5Fit%2FDocuments%2FPhD%2FDevelopment%20Tools%20for%20Scientific%20Computing%20%2D%20Project%20material%20and%20references)]. Using the Rayleigh-Rit procedure [[3](https://sissa-my.sharepoint.com/my?id=%2Fpersonal%2Fglicausi%5Fsissa%5Fit%2FDocuments%2FPhD%2FDevelopment%20Tools%20for%20Scientific%20Computing%20%2D%20Project%20material%20and%20references%2F11%2Dapproximations%2Dfrom%2Da%2Dsubspace%2D1998%2Epdf&parent=%2Fpersonal%2Fglicausi%5Fsissa%5Fit%2FDocuments%2FPhD%2FDevelopment%20Tools%20for%20Scientific%20Computing%20%2D%20Project%20material%20and%20references)] and the Krylov subspace, the number of iteration that it takes to achieve convergence is [[2](https://sissa-my.sharepoint.com/my?id=%2Fpersonal%2Fglicausi%5Fsissa%5Fit%2FDocuments%2FPhD%2FDevelopment%20Tools%20for%20Scientific%20Computing%20%2D%20Project%20material%20and%20references%2F12%2Dkrylov%2Dsubspaces%2D1998%2Epdf&parent=%2Fpersonal%2Fglicausi%5Fsissa%5Fit%2FDocuments%2FPhD%2FDevelopment%20Tools%20for%20Scientific%20Computing%20%2D%20Project%20material%20and%20references)]:\n",
    "$$\\frac{1}{2} \\sqrt{n} \\ln [200 \\sqrt{n}]\\left(\\frac{1}{2} \\sqrt{n} \\ln 200\\right)$$\n",
    "The power method will take:\n",
    "$$n \\ln [100 \\sqrt{n}](n \\ln 100)$$\n",
    "The QR algorithm makes implicit use of Krylov subspace. To introduce the method, without hiding any important detail, let's start with a definition. A subspace $\\mathcal{S^1}$ is said to be invariant if $\\mathcal{S^1}$ is mapped into itself by $\\boldsymbol{A}$ i.e. $\\boldsymbol{A}\\mathcal{S^1} \\subset \\mathcal{S^1} $. In particular, if the subspace $\\mathcal{S^1}$ is the space spanned by the whole set of eigenvector then $\\boldsymbol{A}\\mathcal{S^1} = \\mathcal{S^1} $ [3].\n",
    "\n",
    "Given a new set of orthogonal basis $\\boldsymbol{Q}=[\\boldsymbol{q_1}, \\boldsymbol{q_2}, ..., \\boldsymbol{q_n}] \\in \\mathbb{C}^{n, m} $, with $\\boldsymbol{Q}^H \\boldsymbol{Q}= \\boldsymbol{I}$, they span an invariant subspace if\n",
    "$$\\boldsymbol{R} (\\boldsymbol{Q})=  \\boldsymbol{A}\\boldsymbol{Q}-\\boldsymbol{Q} \\boldsymbol{H}=\\boldsymbol{0}$$ \n",
    "or\n",
    "$$\\boldsymbol{H} = \\boldsymbol{Q} ^H \\boldsymbol{A}\\boldsymbol{Q} $$ \n",
    "$\\boldsymbol{H} $ and $ \\boldsymbol{A}$ shares the same eigenvalues $\\lambda _ i $, and each eigenvector $\\boldsymbol{y}$ of $\\boldsymbol{H}$ determines an eigenvector $\\boldsymbol{Q} \\boldsymbol{y}$ of $\\boldsymbol{A}$. Indeed:\n",
    "$$\\boldsymbol{H} \\boldsymbol{y} =\\boldsymbol{Q} ^H \\boldsymbol{A}\\boldsymbol{Q}  \\boldsymbol{y} =  \\boldsymbol{Q} ^H \\lambda \\boldsymbol{Q}  \\boldsymbol{y} = \\lambda  \\boldsymbol{y} $$\n",
    "\n",
    "\n",
    "Now that the most basic definitions and properties have been introduced, we can present the QR algorithm, which is one of the most important algorithms in eigenvalue computations. It delivers all the eigenvalues, and eventually the eigenvectors of  a given matrix, with an overall complexity of $O(n^3)$ [[4](https://sissa-my.sharepoint.com/personal/glicausi_sissa_it/Documents/PhD/Development%20Tools%20for%20Scientific%20Computing%20-%20Project%20material%20and%20references/chapter4.pdf?CT=1741959188709&OR=ItemsView)\n",
    "]. For simplicity, the dissertation about the QR method will be split into two parts, covered in the following two sections.\n",
    "\n",
    "### Reduction of the symmetric matrix into tridiagonal matrix\n",
    "Even though the QR method, described in thte next section can be applied directly to the full symmetric matrix, it can be shown that the convergence of the method is slow, and each step is really expensive. Thus, in a sort of preprocessing phase, the original matrix $\\boldsymbol{A}$ is reduced to a symmetric tridiagonal matrix $\\boldsymbol{T}$ [4]. The advantages of $\\boldsymbol{T}$ over $\\boldsymbol{A}$ in the iteration process will be discussed later on.\n",
    "\n",
    "There is an intimacy connection between the Krylov subspaces and tridiagonal matrices. there is a distinguished orthonormal basis, $\\boldsymbol{Q}=[\\boldsymbol{q}_1, ...\\boldsymbol{q}_n]$, which is the result of applying the Gram—Schmidt orthonormalizing process to the columns of $K^m(\\boldsymbol{q}_1)$ in the natural order $\\boldsymbol{q}_1, \\boldsymbol{A}\\boldsymbol{q}_1, \\, ....$. Suppose that $\\boldsymbol{q}_1, \\boldsymbol{q}_2, ..., \\boldsymbol{q}_{i-1}$ are given, with $i<n$, then $\\boldsymbol{q}_{i}$ can be obtained by orthonormalize $\\boldsymbol{A}\\boldsymbol{q}_{i-1}$ against  $\\boldsymbol{q}_1, \\boldsymbol{q}_2, ..., \\boldsymbol{q}_{i-1}$ [2]. \n",
    "\n",
    "*When  $K^m(\\boldsymbol{x}^o, \\boldsymbol{A})$ has full rank and how it was described previously,  then $\\boldsymbol{T}=\\boldsymbol{Q} ^H \\boldsymbol{A}\\boldsymbol{Q}$ is an unreduced tridiagonal matrix*[2].\n",
    "$$\n",
    "\\boldsymbol{T}= \\left[\\begin{array}{ccccc}\n",
    "\\alpha_1 & \\beta_1 & & & \\\\\n",
    "\\beta_1 & \\alpha_2 & \\beta_2 & & \\\\\n",
    "& \\beta_2 & \\cdot & \\cdot & \\\\\n",
    "& & \\cdot & \\cdot & \\beta_{n-1} \\\\\n",
    "& & & \\beta_{n-1} & \\alpha_n\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Where $\\alpha_i=\\boldsymbol{q}_i ^H \\boldsymbol{A} \\boldsymbol{q}_i$, and $\\beta_i = \\boldsymbol{q}_{i+1} ^H \\boldsymbol{A} \\boldsymbol{q}_i$.\n",
    "\n",
    "The pseudocode that describes the Basic Lanczos algorithm is shown below [6]. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\overline{\\text { Basic Lanczos algorithm for the computation of an orthonormal }}\\\\\n",
    "&\\text { basis for of the Krylov space } \\mathcal{K}^m(\\mathrm{x})\\\\\n",
    "&\\text { Let } A \\in \\mathbb{F}^{n \\times n} \\text { be Hermitian. This algorithm computes the Lanczos relation (10.13), }\\\\\n",
    "&\\text { i.e., an orthonormal basis } Q_m=\\left[\\mathbf{q}_1, \\ldots, \\mathbf{q}_m\\right] \\text { for } \\mathcal{K}^m(\\mathbf{x}) \\text { where } m \\text { is the smallest index }\\\\\n",
    "&\\text { such that } \\mathcal{K}^m(\\mathbf{x})=\\mathcal{K}^{m+1}(\\mathbf{x}) \\text {, and (the nontrivial elements of) the tridiagonal matrix }\\\\\n",
    "&T_m \\text {. }\\\\\n",
    "&\\mathbf{q}:=\\mathbf{x} /\\|\\mathbf{x}\\| ; \\quad Q_1=[\\mathbf{q}] ;\\\\\n",
    "&\\mathbf{r}:=A \\mathbf{q} \\text {; }\\\\\n",
    "&\\alpha_1:=\\mathbf{q}^* \\mathbf{r} ;\\\\\n",
    "&\\mathbf{r}:=\\mathbf{r}-\\alpha_1 \\mathbf{q} \\text {; }\\\\\n",
    "&\\beta_1:=\\|\\mathbf{r}\\| ;\\\\\n",
    "&\\text { for } j=2,3, \\ldots \\text { do }\\\\\n",
    "&\\mathbf{v}=\\mathbf{q} ; \\quad \\mathbf{q}:=\\mathbf{r} / \\beta_{j-1} ; \\quad Q_j:=\\left[Q_{j-1}, \\mathbf{q}\\right] ;\\\\\n",
    "&\\mathbf{r}:=A \\mathbf{q}-\\beta_{j-1} \\mathbf{v} \\text {; }\\\\\n",
    "&\\alpha_j:=\\mathbf{q}^* \\mathbf{r} \\text {; }\\\\\n",
    "&\\mathbf{r}:=\\mathbf{r}-\\alpha_j \\mathbf{q} ;\\\\\n",
    "&\\beta_j:=\\|\\mathbf{r}\\| \\text {; }\\\\\n",
    "&\\text { if } \\beta_j=0 \\text { then }\\\\\n",
    "&\\text { return }\\left(Q \\in \\mathbb{F}^{n \\times j} ; \\alpha_1, \\ldots, \\alpha_j ; \\beta_1, \\ldots, \\beta_{j-1}\\right)\\\\\n",
    "&\\text { end if }\\\\\n",
    "&\\text { end for }\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Unfortunatly this simple algorithm fails to, when the matrix dimension n becomes larger than about 10 (from our experience). Indeed, deling with finite precision arithmetic makes this method unstable and, as the iterations goes by, loss of orthogonalities between elements of the basis are responsable for the a spurious eigenvalue's multiplicity [5]. Unfortunately, this simple algorithm fails, when the matrix dimension n becomes larger than about 10 (from our experience). Indeed, dealing with finite precision arithmetic makes this method unstable and, as the iterations go by, the loss of orthogonalities between elements of the basis is responsible for the spurious eigenvalue's multiplicity [5]. Examples of this phenomenon, error analyses, and workaround strategies are dealt with exhaustively in [5], [6], but those go beyond the scope of this dissertation. Instead, we limit to picking the useful and relevant results. \n",
    "\n",
    "## Profiling\n",
    "\n",
    "In this section we show the profiling and function's optimization based on the profiling analysys. \n",
    "\n",
    "### About the algorithm\n",
    "\n",
    "\n",
    "Given the symmetry of the matrix $\\boldsymbol{A}$, we choose the algorithm specialized for this kind of matrices, that allowed for the lowest computational complexity. For instance, to ease the convergence of the QR algrithm, a similarity reduction of the original matrices to a \"simpler\" matrix is required. If for a general matrices, the reduction using the Householder reflector leads to a quasi upper triangular Hessemberg matrix. The cost of this reduction amounts to $O(\\frac{14}{3} n^3)$.\n",
    "If the original matrix is symmetric, the Hessemberg matrix must be symmetric too, leading to a tridiagonal Matrix. The algorithm implied for this reduction is the Lanczos algorithm, which scales approximatly as $O(2n^2)$. Apart from this algorithm optimization the Lanczos algorithm does not lend itsel to any kind of optimization. Whenever possible, to improve performance, numpy function and operator are used to improve performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jit, prange\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mline_profiler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m profile, LineProfiler\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import jit, prange\n",
    "from line_profiler import profile, LineProfiler\n",
    "from time import time\n",
    "\n",
    "def Lanczos_PRO_original(A, q, m=None, toll=np.sqrt(np.finfo(float).eps)):\n",
    "    \"\"\"\n",
    "    Perform the Lanczos algorithm for symmetric matrices.\n",
    "    This function computes an orthogonal matrix Q and tridiagonal matrix T such that A ≈ Q * T * Q.T,\n",
    "    where A is a symmetric matrix. The algorithm is useful for finding a few eigenvalues and eigenvectors\n",
    "    of large symmetric matrices.\n",
    "    Args:\n",
    "        A (np.ndarray): A symmetric square matrix of size n x n.\n",
    "        q (np.ndarray): Initial vector of size n.\n",
    "        m (int, optional): Number of eigenvalues to compute. Must be less than or equal to n.\n",
    "                           If None, defaults to the size of A.\n",
    "\n",
    "\n",
    "        tuple: A tuple (Q, alpha, beta) where:\n",
    "\n",
    "\n",
    "            - Q (np.ndarray): Orthogonal matrix of size n x m.\n",
    "\n",
    "\n",
    "            - alpha (np.ndarray): Vector of size m containing the diagonal elements of the tridiagonal matrix.\n",
    "\n",
    "\n",
    "            - beta (np.ndarray): Vector of size m-1 containing the off-diagonal elements of the tridiagonal matrix.\n",
    "    Raises:\n",
    "        ValueError: If the input matrix A is not square or if m is greater than the size of A.\n",
    "    \"\"\"\n",
    "    if m == None:\n",
    "        m = A.shape[0]\n",
    "\n",
    "    if A.shape[0] != A.shape[1]:\n",
    "        raise ValueError(\"Input matrix A must be square.\")\n",
    "\n",
    "    if A.shape[0] != q.shape[0]:\n",
    "        raise ValueError(\"Input vector q must have the same size as the matrix A.\")\n",
    "    q = q / np.linalg.norm(q)\n",
    "    Q = np.array([q])\n",
    "    r = A @ q\n",
    "    alpha = []\n",
    "    beta = []\n",
    "    alpha.append(q @ r)\n",
    "    r = r - alpha[0] * q\n",
    "    beta.append(np.linalg.norm(r))\n",
    "    count = 0\n",
    "    for j in range(1, m):\n",
    "        q = r / beta[j - 1]\n",
    "        for q_basis in Q[:-1]:\n",
    "            if np.abs(q @ q_basis) > toll:\n",
    "                for q_bbasis in Q[:-1]:\n",
    "                    q = q - (q @ q_bbasis) * q_bbasis\n",
    "                    count += 1\n",
    "                break\n",
    "        q = q / np.linalg.norm(q)\n",
    "        Q = np.vstack((Q, q))\n",
    "        r = A @ q - beta[j - 1] * Q[j - 1]\n",
    "        alpha.append(q @ r)\n",
    "        r = r - alpha[j] * q\n",
    "        beta.append(np.linalg.norm(r))\n",
    "\n",
    "        if np.abs(beta[j]) < 1e-15:\n",
    "            return Q, alpha, beta[:-1]\n",
    "\n",
    "\n",
    "    return Q, alpha, beta[:-1]\n",
    "\n",
    "\n",
    "#@jit(parallel=True)  \n",
    "def Lanczos_PRO(A, q,  m=None, toll=np.sqrt(np.finfo(float).eps)):\n",
    "    \"\"\"\n",
    "    Lanczos algorithm for symmetric matrices\n",
    "    :param A: symmetric matrix square matrix of size n\n",
    "    :param q: initial vector\n",
    "    :param m: number of eigenvalues to compute. m<=n\n",
    "    :param toll: tolerance for the computation\n",
    "\n",
    "    :return: Q, alpha, beta\n",
    "    Q: orthogonal matrix of size n x m\n",
    "    alpha: vector of size m. Diagonal of the tridiagonal matrix\n",
    "    beta: vector of size m-1. Upper diagonal of the tridiagonal matrix\n",
    "    \"\"\"\n",
    "    if m==None:\n",
    "        m=A.shape[0]\n",
    "        \n",
    "    q=q/np.linalg.norm(q)\n",
    "    #Q=np.array([q])\n",
    "    Q = np.zeros((m, A.shape[0]))\n",
    "    Q[0] = q\n",
    "    r=A@q\n",
    "    alpha=[]\n",
    "    beta=[]\n",
    "    alpha.append(q@r)\n",
    "    r=r-alpha[0]*q\n",
    "    beta.append(np.linalg.norm(r))\n",
    "\n",
    "    for j in range(1, m):\n",
    "        q=r/beta[j-1]\n",
    "        if np.any(np.abs(q@Q[:j-1].T)>toll):\n",
    "            for q_bbasis in Q[:j-1]:\n",
    "                q = q - (q @ q_bbasis) * q_bbasis\n",
    "            # for i in prange(j-1):\n",
    "            #     q = q - (q @ Q[i]) * Q[i]\n",
    "\n",
    "        q=q/np.linalg.norm(q)\n",
    "        Q[j]=q\n",
    "        r=A@q-beta[j-1]*Q[j-1]\n",
    "        alpha.append(q@r)\n",
    "        r=r-alpha[j]*q\n",
    "        beta.append(np.linalg.norm(r))\n",
    "\n",
    "        if np.abs(beta[j])<1e-15:\n",
    "            \n",
    "            return Q, alpha, beta[:-1]\n",
    "    return Q, alpha, beta[:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 4.42064 s\n",
      "File: /tmp/ipykernel_45483/3254536551.py\n",
      "Function: Lanczos_PRO_original at line 6\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     6                                           def Lanczos_PRO_original(A, q, m=None, toll=np.sqrt(np.finfo(float).eps)):\n",
      "     7                                               \"\"\"\n",
      "     8                                               Perform the Lanczos algorithm for symmetric matrices.\n",
      "     9                                               This function computes an orthogonal matrix Q and tridiagonal matrix T such that A ≈ Q * T * Q.T,\n",
      "    10                                               where A is a symmetric matrix. The algorithm is useful for finding a few eigenvalues and eigenvectors\n",
      "    11                                               of large symmetric matrices.\n",
      "    12                                               Args:\n",
      "    13                                                   A (np.ndarray): A symmetric square matrix of size n x n.\n",
      "    14                                                   q (np.ndarray): Initial vector of size n.\n",
      "    15                                                   m (int, optional): Number of eigenvalues to compute. Must be less than or equal to n.\n",
      "    16                                                                      If None, defaults to the size of A.\n",
      "    17                                           \n",
      "    18                                           \n",
      "    19                                                   tuple: A tuple (Q, alpha, beta) where:\n",
      "    20                                           \n",
      "    21                                           \n",
      "    22                                                       - Q (np.ndarray): Orthogonal matrix of size n x m.\n",
      "    23                                           \n",
      "    24                                           \n",
      "    25                                                       - alpha (np.ndarray): Vector of size m containing the diagonal elements of the tridiagonal matrix.\n",
      "    26                                           \n",
      "    27                                           \n",
      "    28                                                       - beta (np.ndarray): Vector of size m-1 containing the off-diagonal elements of the tridiagonal matrix.\n",
      "    29                                               Raises:\n",
      "    30                                                   ValueError: If the input matrix A is not square or if m is greater than the size of A.\n",
      "    31                                               \"\"\"\n",
      "    32         1       2550.0   2550.0      0.0      if m == None:\n",
      "    33         1       4581.0   4581.0      0.0          m = A.shape[0]\n",
      "    34                                           \n",
      "    35         1       1478.0   1478.0      0.0      if A.shape[0] != A.shape[1]:\n",
      "    36                                                   raise ValueError(\"Input matrix A must be square.\")\n",
      "    37                                           \n",
      "    38         1        764.0    764.0      0.0      if A.shape[0] != q.shape[0]:\n",
      "    39                                                   raise ValueError(\"Input vector q must have the same size as the matrix A.\")\n",
      "    40         1     189095.0 189095.0      0.0      q = q / np.linalg.norm(q)\n",
      "    41         1      15378.0  15378.0      0.0      Q = np.array([q])\n",
      "    42         1    3445585.0    3e+06      0.1      r = A @ q\n",
      "    43         1       2535.0   2535.0      0.0      alpha = []\n",
      "    44         1       1160.0   1160.0      0.0      beta = []\n",
      "    45         1      46579.0  46579.0      0.0      alpha.append(q @ r)\n",
      "    46         1      50491.0  50491.0      0.0      r = r - alpha[0] * q\n",
      "    47         1     101868.0 101868.0      0.0      beta.append(np.linalg.norm(r))\n",
      "    48         1        911.0    911.0      0.0      count = 0\n",
      "    49      1000     710682.0    710.7      0.0      for j in range(1, m):\n",
      "    50       999    3709404.0   3713.1      0.1          q = r / beta[j - 1]\n",
      "    51    251149  154186390.0    613.9      3.5          for q_basis in Q[:-1]:\n",
      "    52    250645 1192865728.0   4759.2     27.0              if np.abs(q @ q_basis) > toll:\n",
      "    53    248846  156721491.0    629.8      3.5                  for q_bbasis in Q[:-1]:\n",
      "    54    248351 1769505180.0   7125.0     40.0                      q = q - (q @ q_bbasis) * q_bbasis\n",
      "    55    248351   89655317.0    361.0      2.0                      count += 1\n",
      "    56       495     393587.0    795.1      0.0                  break\n",
      "    57       999   23866908.0  23890.8      0.5          q = q / np.linalg.norm(q)\n",
      "    58       999  604587063.0 605192.3     13.7          Q = np.vstack((Q, q))\n",
      "    59       999  382711807.0 383094.9      8.7          r = A @ q - beta[j - 1] * Q[j - 1]\n",
      "    60       999    7923029.0   7931.0      0.2          alpha.append(q @ r)\n",
      "    61       999    5365710.0   5371.1      0.1          r = r - alpha[j] * q\n",
      "    62       999   20907494.0  20928.4      0.5          beta.append(np.linalg.norm(r))\n",
      "    63                                           \n",
      "    64       999    3651429.0   3655.1      0.1          if np.abs(beta[j]) < 1e-15:\n",
      "    65                                                       return Q, alpha, beta[:-1]\n",
      "    66                                           \n",
      "    67                                           \n",
      "    68         1      11316.0  11316.0      0.0      return Q, alpha, beta[:-1]\n",
      "\n",
      "Total time: 2.17926 s\n",
      "File: /tmp/ipykernel_45483/3254536551.py\n",
      "Function: Lanczos_PRO at line 72\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    72                                           def Lanczos_PRO(A, q,  m=None, toll=np.sqrt(np.finfo(float).eps)):\n",
      "    73                                               \"\"\"\n",
      "    74                                               Lanczos algorithm for symmetric matrices\n",
      "    75                                               :param A: symmetric matrix square matrix of size n\n",
      "    76                                               :param q: initial vector\n",
      "    77                                               :param m: number of eigenvalues to compute. m<=n\n",
      "    78                                               :param toll: tolerance for the computation\n",
      "    79                                           \n",
      "    80                                               :return: Q, alpha, beta\n",
      "    81                                               Q: orthogonal matrix of size n x m\n",
      "    82                                               alpha: vector of size m. Diagonal of the tridiagonal matrix\n",
      "    83                                               beta: vector of size m-1. Upper diagonal of the tridiagonal matrix\n",
      "    84                                               \"\"\"\n",
      "    85         1        931.0    931.0      0.0      if m==None:\n",
      "    86         1       2196.0   2196.0      0.0          m=A.shape[0]\n",
      "    87                                                   \n",
      "    88         1      49542.0  49542.0      0.0      q=q/np.linalg.norm(q)\n",
      "    89                                               #Q=np.array([q])\n",
      "    90         1      43730.0  43730.0      0.0      Q = np.zeros((m, A.shape[0]))\n",
      "    91         1       7902.0   7902.0      0.0      Q[0] = q\n",
      "    92         1      91293.0  91293.0      0.0      r=A@q\n",
      "    93         1        383.0    383.0      0.0      alpha=[]\n",
      "    94         1        248.0    248.0      0.0      beta=[]\n",
      "    95         1       6858.0   6858.0      0.0      alpha.append(q@r)\n",
      "    96         1      10192.0  10192.0      0.0      r=r-alpha[0]*q\n",
      "    97         1      21388.0  21388.0      0.0      beta.append(np.linalg.norm(r))\n",
      "    98                                           \n",
      "    99      1000     508241.0    508.2      0.0      for j in range(1, m):\n",
      "   100       999    3305123.0   3308.4      0.2          q=r/beta[j-1]\n",
      "   101       999  141874044.0 142016.1      6.5          if np.any(np.abs(q@Q[:j-1].T)>toll):\n",
      "   102    248846  150389778.0    604.3      6.9              for q_bbasis in Q[:j-1]:\n",
      "   103    248351 1633125885.0   6575.9     74.9                  q = q - (q @ q_bbasis) * q_bbasis\n",
      "   104                                                       # for i in prange(j-1):\n",
      "   105                                                       #     q = q - (q @ Q[i]) * Q[i]\n",
      "   106                                           \n",
      "   107       999   21728759.0  21750.5      1.0          q=q/np.linalg.norm(q)\n",
      "   108       999    9238718.0   9248.0      0.4          Q[j]=q\n",
      "   109       999  190603293.0 190794.1      8.7          r=A@q-beta[j-1]*Q[j-1]\n",
      "   110       999    3430168.0   3433.6      0.2          alpha.append(q@r)\n",
      "   111       999    4648981.0   4653.6      0.2          r=r-alpha[j]*q\n",
      "   112       999   17478743.0  17496.2      0.8          beta.append(np.linalg.norm(r))\n",
      "   113                                           \n",
      "   114       999    2679257.0   2681.9      0.1          if np.abs(beta[j])<1e-15:\n",
      "   115                                                       \n",
      "   116                                                       return Q, alpha, beta[:-1]\n",
      "   117         1      11600.0  11600.0      0.0      return Q, alpha, beta[:-1]\n",
      "\n",
      "Optimized function time: 2.3847594261169434\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "size = 1000\n",
    "A = np.random.rand(size, size)\n",
    "A = (A + A.T) / 2\n",
    "A=np.array(A, dtype=float)\n",
    "q = np.random.rand(size)\n",
    "lp = LineProfiler()\n",
    "lp.add_function(Lanczos_PRO_original)\n",
    "result = lp.run('Lanczos_PRO_original(A, q)')\n",
    "#lp.print_stats()\n",
    "#Q, alpha, beta = Lanczos_PRO(A, q)\n",
    "\n",
    "\n",
    "\n",
    "t_s=time()\n",
    "lp.add_function(Lanczos_PRO)\n",
    "result = lp.run('Lanczos_PRO(A, q)')\n",
    "lp.print_stats()\n",
    "\n",
    "t_e=time()\n",
    "print(f\"Optimized function time: {t_e-t_s}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the line profiler we can see that the most expensive operation is associated with reorthogonalization. To improve this a slight modified version was implemented. Also the np.vstack operation was quite costly, so it was replaced with a simple assignment. This little adjustment allowed to almost halve the cost of the Lanczos Algorithm. In the final version the decorator jit, from numba packeage was additionally used. The only portion of code that could be paralleliza using prange is the for associated with reorthogonalization, but this slows down the performance.\n",
    "\n",
    "### QR algorithm\n",
    "\n",
    "The QR algorithm, computed with the Given rotation is difficult to be parallelized. By exploiting the tridiagonal shape fo the matrix, it can be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m lp_QR \u001b[38;5;241m=\u001b[39m LineProfiler()\n\u001b[1;32m     86\u001b[0m lp_QR\u001b[38;5;241m.\u001b[39madd_function(QR_method)\n\u001b[0;32m---> 87\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mlp_QR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQR_method(T)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m lp_QR\u001b[38;5;241m.\u001b[39madd_function(QR_method_optimized)\n\u001b[1;32m     91\u001b[0m result \u001b[38;5;241m=\u001b[39m lp_QR\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQR_method_optimized(T)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/line_profiler/line_profiler.py:178\u001b[0m, in \u001b[0;36mLineProfiler.run\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__main__\u001b[39;00m\n\u001b[1;32m    177\u001b[0m main_dict \u001b[38;5;241m=\u001b[39m __main__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/line_profiler/line_profiler.py:185\u001b[0m, in \u001b[0;36mLineProfiler.runctx\u001b[0;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_by_count()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_by_count()\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mQR_method\u001b[0;34m(A_copy, tol, max_iter)\u001b[0m\n\u001b[1;32m     26\u001b[0m     R\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39meye(A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     27\u001b[0m     R[i: i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m, i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[Matrix_trigonometry[i, \u001b[38;5;241m0\u001b[39m], Matrix_trigonometry[i, \u001b[38;5;241m1\u001b[39m]], [\u001b[38;5;241m-\u001b[39mMatrix_trigonometry[i, \u001b[38;5;241m1\u001b[39m], Matrix_trigonometry[i, \u001b[38;5;241m0\u001b[39m]]])\n\u001b[0;32m---> 28\u001b[0m     Q \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;129m@R\u001b[39m\n\u001b[1;32m     29\u001b[0m A\u001b[38;5;241m=\u001b[39mA\u001b[38;5;129m@Q\u001b[39m\n\u001b[1;32m     30\u001b[0m A \u001b[38;5;241m=\u001b[39m A \u001b[38;5;241m@\u001b[39m Q  \u001b[38;5;66;03m# Update A\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def QR_method(A_copy, tol=1e-10, max_iter=100):\n",
    "    A = A_copy.copy()\n",
    "    iter = 0\n",
    "    Q = np.eye(A.shape[0])\n",
    "    \n",
    "    # Correctly preallocate as a 2D array (n-1, 2)\n",
    "    Matrix_trigonometry = np.zeros((A.shape[0] - 1, 2))\n",
    "\n",
    "    while np.linalg.norm(np.diag(A, -1), np.inf) > tol and iter < max_iter:\n",
    "        # Compute Givens rotation\n",
    "        for i in range(A.shape[0] - 1):\n",
    "            c = A[i, i] / np.sqrt(A[i, i] ** 2 + A[i + 1, i] ** 2)\n",
    "            s = -A[i + 1, i] / np.sqrt(A[i, i] ** 2 + A[i + 1, i] ** 2)\n",
    "            Matrix_trigonometry[i, :] = [c, s]  \n",
    "            R=np.eye(A.shape[0])\n",
    "            # Apply the Givens rotation to A (modify in place)\n",
    "            R[i:i+2, i:i+2] = np.array([[c, -s], [s, c]])\n",
    "            A= R @ A\n",
    "            A[i+1, i] = 0 \n",
    "\n",
    "\n",
    "        Q=np.eye(A.shape[0])\n",
    "        i=0\n",
    "        Q[0:2, 0:2]=np.array([[Matrix_trigonometry[i, 0], Matrix_trigonometry[i, 1]], [-Matrix_trigonometry[i, 1], Matrix_trigonometry[i, 0]]])\n",
    "        for i in range(1, A.shape[0]-1):\n",
    "            R=np.eye(A.shape[0])\n",
    "            R[i: i+2, i:i+2]=np.array([[Matrix_trigonometry[i, 0], Matrix_trigonometry[i, 1]], [-Matrix_trigonometry[i, 1], Matrix_trigonometry[i, 0]]])\n",
    "            Q = Q@R\n",
    "        A=A@Q\n",
    "        A = A @ Q  # Update A\n",
    "        iter += 1\n",
    "\n",
    "    if iter == max_iter:\n",
    "        print(\"QR method did not converge\")\n",
    "\n",
    "    return np.diag(A), Q\n",
    "\n",
    "#@jit(nopython=True)\n",
    "def QR_method_optimized(A_copy, tol=1e-10, max_iter=100):\n",
    "    A = A_copy.copy()\n",
    "    iter = 0\n",
    "    Q = np.eye(A.shape[0])\n",
    "    \n",
    "    # Correctly preallocate as a 2D array (n-1, 2)\n",
    "    Matrix_trigonometry = np.zeros((A.shape[0] - 1, 2))\n",
    "    d=np.zeros(A.shape[0])\n",
    "    #while np.linalg.norm((np.diag(A, -1)), np.inf) > tol and iter < max_iter:\n",
    "    while np.linalg.norm((np.diag(A, 0)-d)/np.diag(A, 0), np.inf) > tol and iter < max_iter:\n",
    "        # Compute Givens rotation\n",
    "        d=np.diag(A, 0)\n",
    "        for i in range(A.shape[0] - 1):\n",
    "            c = A[i, i] / np.sqrt(A[i, i] ** 2 + A[i + 1, i] ** 2)\n",
    "            s = -A[i + 1, i] / np.sqrt(A[i, i] ** 2 + A[i + 1, i] ** 2)\n",
    "            Matrix_trigonometry[i, :] = [c, s]  \n",
    "\n",
    "            # Apply the Givens rotation to A (modify in place)\n",
    "            R = np.array([[c, -s], [s, c]])\n",
    "            A[i:i+2, i:] = R @ A[i:i+2, i:]\n",
    "            A[i+1, i] = 0 \n",
    "\n",
    "        # Construct full Q matrix from stored Givens rotations\n",
    "        Q = np.eye(A.shape[0])\n",
    "        for i in range(A.shape[0] - 1):\n",
    "            R=np.array([[Matrix_trigonometry[i, 0], Matrix_trigonometry[i, 1]], [-Matrix_trigonometry[i, 1], Matrix_trigonometry[i, 0]]])\n",
    "            Q[:, i:i+2] = Q[:, i:i+2]@R\n",
    "        A = A @ Q  # Update A\n",
    "        iter += 1\n",
    "\n",
    "    return np.diag(A), Q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "size = 300\n",
    "A = np.random.rand(size, size)\n",
    "A = (A + A.T) / 2\n",
    "x0 = np.random.rand(size)\n",
    "\n",
    "q, alpha, beta = Lanczos_PRO(A, x0, size)\n",
    "\n",
    "T=np.diag(alpha)+np.diag(beta, k=1)+np.diag(beta, k=-1)\n",
    "QR_method_optimized(T)\n",
    "lp_QR = LineProfiler()\n",
    "lp_QR.add_function(QR_method)\n",
    "result = lp_QR.run('QR_method(T)')\n",
    "\n",
    "\n",
    "lp_QR.add_function(QR_method_optimized)\n",
    "result = lp_QR.run('QR_method_optimized(T)')\n",
    "lp_QR.print_stats()\n",
    "# t_s=time()\n",
    "# result= QR_method_optimized(T)\n",
    "# t_e=time()\n",
    "# print(f\"Optimized function time: {t_e-t_s}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first simple optimization concern the memory. The Rotation matrix at each iteration is completly defined by the value of the sine and cosine (s, c) of the rotation angle, which can be assembled in a $2 \\times 2 $ matrix, and the position of this block in the $n \\times n $ rotation matrix $G(i, \\theta_i)$ matrix, with $i=0, 1, 2, ..., n-2$. Therefore rather than storing $n-1$ $n \\times n$ rotation matrix, it is possible store all the inforamtion they carry out in a $(n-1)\\times 2$ matrix in which the $i$-th row has the value of sine and cosine associated with the matrix $G(i, \\theta_i)$.\n",
    "\n",
    "Profiling the QR method shows as the most expensive operatation are the matrix matrix multiplication, which adds up to the 92% of the function total run. Knowing the structure of the matrix $G(i, \\theta_i)$ allow to reduce the size of the matrix multiplication (see lines 53 and 60). A final 98% speed up of the code was achieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m      3\u001b[0m A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(size, size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "size = 5000\n",
    "A = np.random.rand(size, size)\n",
    "A = (A + A.T) / 2\n",
    "x0 = np.random.rand(size)\n",
    "q, alpha, beta = Lanczos_PRO(A, x0, size)\n",
    "\n",
    "t_s=time()\n",
    "np.linalg.eig(A)\n",
    "t_e=time()\n",
    "\n",
    "print(f\"{t_e-t_s: .4e}\")\n",
    "\n",
    "# T=np.diag(alpha)+np.diag(beta, k=1)+np.diag(beta, k=-1)\n",
    "# t_s=time()\n",
    "# result= QR_method_optimized(T, max_iter=5000, tol=1e-6)\n",
    "# t_e=time()\n",
    "# print(f\"Optimized function time: {t_e-t_s}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "\n",
    "## References\n",
    "[1] [Panju, Maysum. \"Iterative methods for computing eigenvalues and eigenvectors.\" arXiv preprint arXiv:1105.1185 (2011).](https://sissa-my.sharepoint.com/:b:/g/personal/glicausi_sissa_it/EeImU3eEs5hNoEF7hEnScxABpd0oMn5zOS8y-A3UJLoAGw?e=OO7IoE)\n",
    "\n",
    "[2] [Parlett, Beresford N. The symmetric eigenvalue problem - Chapter 12. Society for Industrial and Applied Mathematics, 1998.](https://sissa-my.sharepoint.com/my?id=%2Fpersonal%2Fglicausi%5Fsissa%5Fit%2FDocuments%2FPhD%2FDevelopment%20Tools%20for%20Scientific%20Computing%20%2D%20Project%20material%20and%20references%2F12%2Dkrylov%2Dsubspaces%2D1998%2Epdf&parent=%2Fpersonal%2Fglicausi%5Fsissa%5Fit%2FDocuments%2FPhD%2FDevelopment%20Tools%20for%20Scientific%20Computing%20%2D%20Project%20material%20and%20references)\n",
    "\n",
    "[3] [Parlett, Beresford N. The symmetric eigenvalue problem - Chapter 11. Society for Industrial and Applied Mathematics, 1998.](https://sissa-my.sharepoint.com/my?id=%2Fpersonal%2Fglicausi%5Fsissa%5Fit%2FDocuments%2FPhD%2FDevelopment%20Tools%20for%20Scientific%20Computing%20%2D%20Project%20material%20and%20references%2F11%2Dapproximations%2Dfrom%2Da%2Dsubspace%2D1998%2Epdf&parent=%2Fpersonal%2Fglicausi%5Fsissa%5Fit%2FDocuments%2FPhD%2FDevelopment%20Tools%20for%20Scientific%20Computing%20%2D%20Project%20material%20and%20references)\n",
    "\n",
    "[4] [Arbenz, Peter. Lecture Notes on Solving Large Scale Eigenvalue Problems - Chapter 4. Computer Science Department, ETH Zürich, Spring semester 2016.](https://sissa-my.sharepoint.com/personal/glicausi_sissa_it/Documents/PhD/Development%20Tools%20for%20Scientific%20Computing%20-%20Project%20material%20and%20references/chapter4.pdf?CT=1741959188709&OR=ItemsView)\n",
    "\n",
    "[5] [Parlett, Beresford N. The symmetric eigenvalue problem - Chapter 13. Society for Industrial and Applied Mathematics, 1998.](https://sissa-my.sharepoint.com/personal/glicausi_sissa_it/Documents/PhD/Development%20Tools%20for%20Scientific%20Computing%20-%20Project%20material%20and%20references/13-lanczos-algorithms-1998.pdf?CT=1741964477462&OR=ItemsView)\n",
    "\n",
    "[6] [Arbenz, Peter. Lecture Notes on Solving Large Scale Eigenvalue Problems - Chapter 10. Computer Science Department, ETH Zürich, Spring semester 2016.](https://sissa-my.sharepoint.com/personal/glicausi_sissa_it/Documents/PhD/Development%20Tools%20for%20Scientific%20Computing%20-%20Project%20material%20and%20references/chapter10.pdf?CT=1741959176514&OR=ItemsView)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
